{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peiyulan/From-Images-to-Text-Working-with-OCR/blob/main/ImageToTextOCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Welcome: From Images to Text: Working with OCR**\n"
      ],
      "metadata": {
        "id": "5LAarKZlJZBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "This course provides an overview of Optical Character Recognition (OCR), an image processing technique for extracting text from images.\n",
        "\n",
        "The resource covers the following sections to help you learn and start to apply OCR in research practice:\n",
        "\n",
        "- What is text extraction and OCR?\n",
        "- What are the real-world application of OCR?\n",
        "- What challenges might you face when using OCR and how can you address them?\n",
        "\n",
        "We will learn the fundamental concepts and implement OCR techniques through the following activities:\n",
        "\n",
        "- Activity 1: Write down your OCR Workflow\n",
        "- Activity 2: Inspect the Files\n",
        "- Activity 3: Online OCR Engine\n",
        "- Activity 4: Clean Your Text with Regex\n",
        "- Activity 5: OCR Tesseract Module with Python or R\n",
        "- Activity 6: HTR Handwritten Text Recognition"
      ],
      "metadata": {
        "id": "3hm6uw_UaSGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> üëã New to Google Colab and Python? No worries! Let's get you started by running the code snippet below to make sure everything is working properly for you. It's a great way to take your first steps into coding!"
      ],
      "metadata": {
        "id": "yjBHVUNPnb7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Enter your name and press enter:\")\n",
        "name = input()\n",
        "print(\"\\r\")\n",
        "print(\"Hello {}, welcome to the CDCS Image to Text with OCR workshop!\".format(name))"
      ],
      "metadata": {
        "id": "9cH3wNj1nWx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OgyNjtzloQ1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is OCR?**"
      ],
      "metadata": {
        "id": "2sH-oL7VMt40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **O**ptical¬†**C**haracter¬†**R**ecognition\n",
        "- OCR is a technique to process images of text, such as written or printed documents, and produce **machine-readable**¬†documents.\n",
        "- Machine-readable documents are encoded in formats that computers can process, which allow the text to be searched, edited and analysed computationally"
      ],
      "metadata": {
        "id": "Gk-DcHSGO9RT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real-world examples of OCR\n",
        "\n",
        "- Scanning your passport at the airport\n",
        "- Generate machine-readable text for text-to-speech technology.\n",
        "- Making digitalised physical archives searchable.‚Äã\n",
        "- Creating a dataset of for text mining or\n",
        "text analysis.‚Äã\n",
        "\n"
      ],
      "metadata": {
        "id": "u4Xf8PprU1em"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OCR Workflow\n",
        "\n",
        "OCR is a multi-step process and we'll examine this as we move through the lesson.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   **Document selection**: An image or text document needs to be selected to scan.\n",
        "2.   **Scanning**: The images are then scanned by OCR software. This generates a machine-readable text output.\n",
        "3.   **Cleaning** :Once you have an OCR output, extra steps to clean the files might be need to improve accuracy.\n",
        "4.   **Saving files**: the machine-readable documents can be saved and are to use.\n",
        "\n"
      ],
      "metadata": {
        "id": "wiE18XX8Uz0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úèÔ∏è *Activity 1: Write Down Your OCR Workflow*  \n",
        "\n",
        "\n",
        "- Identify a dataset (images of text)\n",
        "that you might use in your research‚Äã.\n",
        "\n",
        "- Write the steps to obtained encoded\n",
        "text from your dataset.‚Äã\n",
        "\n",
        "- Identify potential source of errors or\n",
        "issues in each step, and discuss how\n",
        "you might address them.‚Äã\n",
        "\n",
        "- Share your dataset, workflow, and\n",
        "plan with your small group.‚Äã\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7Wcz0_MXQ4kV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Challange and Error of OCR**"
      ],
      "metadata": {
        "id": "xbsjNGskfWhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, text recognition is not a perfect process, and you are likely to encounter problems or errors in the text outputs.\n",
        "\n",
        "The accuracy of OCR can be limited by the **OCR engine capability**:\n",
        "- file size\n",
        "- format of the input files\n",
        "- text orientation\n",
        "- language it can process\n",
        "\n",
        "It also depends on **dataset quality, formatting of the origianl documents:**\n",
        "- Human errors and typos‚Äã\n",
        "- Age and damage (stained or blurry)‚Äã\n",
        "- Mixed text and images, or multiple\n",
        "languages‚Äã\n",
        "- Cursive handwriting‚Äã"
      ],
      "metadata": {
        "id": "zsDtYhJ7myzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ways to improve OCR accuracy**\n",
        "\n",
        "While the accuracy or OCR will never be 100%, there are ways to reduce errors and imrove OCR accuracy:\n",
        "\n",
        "- Select good quality dataset to begin with‚Äã\n",
        "\n",
        "- Pre-process your dataset to improve its quality‚Äã\n",
        "\n",
        "- Correct errors in OCR-produced files\n",
        "\n",
        "- Improve OCR engine capability‚Äã"
      ],
      "metadata": {
        "id": "V-4lKsJUasXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "o-oDcG13lR89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Selection\n",
        "\n",
        "Selecting good quality data to start with means that you don‚Äôt need to edit and process your image too much from the beginning. Key considerations include:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C-jD-ZQnTcLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image resolution**\n",
        "\n",
        "Set a minimum DPI metric. DPI means ‚ÄòDots Per Inch‚Äô and means that every inch of an image contains a certain number of dots of ink. 300 DPI is often used as a benchmark for good quality printing reproducibility for photographs, but this may vary.  \n",
        "\n",
        "**Types of error**\n",
        "\n",
        "By checking the documents or sample documents, you can identify the patterns documents that might cause errors during scanning and reduece the work required for clearning the files afterwards.\n",
        "\n",
        "Some \"patterned\" or \"predictable\" error can be fixed commutationally using software or programming patches:\n",
        "\n",
        "*   **Characters that appear similar** can be misrecognised, for example ‚Äòcl‚Äô and ‚Äòd‚Äô, or ‚Äòrn‚Äô and ‚Äòm‚Äô, which results in the incorrect substitution of a letter or letters. E.g. ‚Äòclean‚Äô becomes recognised as ‚Äòdean‚Äô.\n",
        "*   **Different letter forms**, such as in some older historical materials a different form of 's' is used. This is called a long s and looks like this '≈ø', which is often mis-recognised as an 'f' character.\n",
        "E.g. ≈øleeve [sleeve] -> fleeve\n",
        "\n",
        "\n",
        "Some error is unpredictable and therefore more difficult to be tackeled, such as:\n",
        "- Age and damage of the files (stained or blurry)\n",
        "- Human error such as typo‚Äãs and smelling variations\n",
        "- Mixed formating, text and images\n",
        "- Cursive handwriting‚Äã\n"
      ],
      "metadata": {
        "id": "eAdl_pmPkFiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úèÔ∏è *Activity 2: Inspect the Files*  \n",
        "\n",
        "*   Identify issues you might encounter when processing the following documents.  ‚Äã\n",
        "- Are there any steps you could take to\n",
        "preprocessing the document that might\n",
        "improve the output accuracy?‚Äã\n",
        "\n",
        "‚Äã\n"
      ],
      "metadata": {
        "id": "XJnrZo-EYr6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/DCS-training/Image-to-Tech-Text-Extraction/blob/main/Github%20Images/Image1.jpg?raw=true)\n"
      ],
      "metadata": {
        "id": "2TnaBMHr0BAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document 1: image above from the Scottish Session Papers collection held by the University of Edinburgh, shelfmark [EUL0011](https://librarylabs.ed.ac.uk/iiif/uv/?manifest=https://librarylabs.ed.ac.uk/iiif/manifest/sessionpapers/volumes/EUL0011.json#?c=0&m=0&s=0&cv=21&xywh=-510%2C0%2C9022%2C5263) under a [CC BY 3.0](https://creativecommons.org/licenses/by/3.0/) licence."
      ],
      "metadata": {
        "id": "JwrEsB021DBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/DCS-training/Image-to-Tech-Text-Extraction/blob/main/Github%20Images/Image2.jpg?raw=true)"
      ],
      "metadata": {
        "id": "YGNsId7007AV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document 2: image above from the Scottish Session Papers collection held by the University of Edinburgh, shelfmark [EUL0281](https://librarylabs.ed.ac.uk/iiif/uv/?manifest=https://librarylabs.ed.ac.uk/iiif/manifest/sessionpapers/volumes/EUL0281.json#?c=0&m=0&s=0&cv=0&xywh=-2584%2C-252%2C8636%2C5038) under a [CC BY 3.0](https://creativecommons.org/licenses/by/3.0/) licence."
      ],
      "metadata": {
        "id": "qEuMQOuo1PsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre-processing**\n",
        "\n",
        "Once you identify the dataset, preparing your files for scanning can help to produce better outputs after scanning. There are a few ways to do this:"
      ],
      "metadata": {
        "id": "mvutqiSaQMc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image colour**\n",
        "\n",
        "The colour of your images can impact text quality too; colour images can be used, but ensure there is a sharp contrast between the background page and the text itself (for example black text on white or cream paper).\n",
        "**Text orientation:**\n",
        "The orientation of your text is also important in text recognition as the letters that your OCR engine tries to match against will be the ‚Äòright‚Äô way up and straight on the page, so making sure that the text in the documents you upload is ‚Äòstraight on‚Äô in the document or image will produce the best results.\n",
        "\n",
        "**File format:**\n",
        "\n",
        "Some OCR engines will only accept certain file formats, so it is best to check this in advance; some will only use image files, whereas others will work on PDF files. Common image file types are TIFF, JPEG and PNG.\n",
        "\n",
        "TIFF files are lossless files, meaning that no image quality or information is lost; this means that TIFF images are usually high quality but also much larger file sizes. PNGs are also lossless, although TIFF files would be preferred over PNGs for OCR.\n",
        "\n",
        "JPEGs are a lossy format type, meaning that the image is compressed to create a smaller file size. OCR engines can work with JPEGs, but there may be a loss of image quality that can impact the text generation."
      ],
      "metadata": {
        "id": "1SYT_XJpc12A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scanning\n",
        "\n",
        "Now we're going to try some OCR ourselves with out-of-the-box options."
      ],
      "metadata": {
        "id": "8wc8tAY7uDf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úèÔ∏è *Activity 3: Online OCR Engine*  \n",
        " Choose a pdf that contains text and try uploading it into some of these online programmes:\n",
        "*   https://tools.pdf24.org/en/ocr-pdf   \n",
        "*   https://www.onlineocr.net/   \n",
        "*   https://www.sodapdf.com/ocr-pdf/   \n",
        "*   https://www.sejda.com/ocr-pdf   \n",
        "*   https://ocr.space/   \n",
        "*   https://avepdf.com/pdf-ocr   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a089BvORcV43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Compare your results - which performed best? What are the limitations of these options?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fJ28M7EXeUpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cleaning**\n",
        "\n",
        "**Manually remove errors**\n",
        "\n",
        "Read through the text and manually change what needs to be changed ‚Äì this is one option to create a high-quality text with your corrections, however, it can be time and effort consuming\n",
        "\n",
        "\n",
        "**Patches and Machine Learning techniques**:\n",
        "Certain software have an embedded lexicon or dictionary that can be used to identify incorrect vocabulary and correct likely words based on the lexicon and calculated probability of correct words.\n",
        "\n",
        "**Regex**\n",
        "\n",
        "Regex (or regular expressions) is a type of shorthand code you can use to specify which parts of text you would like to target and how you would like to change these."
      ],
      "metadata": {
        "id": "jcbm0j_3bSE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OCR has been developing at a significant rate over the past twenty years and newer OCR is faster and more accurate than older software, but it is still liable to errors."
      ],
      "metadata": {
        "id": "GICMHGOLYnJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regex (Regular Expressions)\n",
        "\n",
        "Regular expressions or regex are a way to identify and match patterns via code and is used in a range of different programming environments.\n",
        "\n",
        "Regex is a powerful way to find, manage and transform your data and files. It uses sequences of characters to define a search to match strings. You can use regex to:\n",
        "*   Match types of characters (e.g. ‚Äòupper case letters‚Äô, ‚Äòdigits‚Äô, ‚Äòspaces‚Äô, etc.)  \n",
        "*   Match patterns that repeat any number of times\n",
        "\n"
      ],
      "metadata": {
        "id": "lw3hTn6Qbflm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are far too many to remember off the top of your head, so online cheat sheets are your best option."
      ],
      "metadata": {
        "id": "zADs6X-ocJm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out the following webpages for different patterns:\n",
        "\n",
        "*   [Cleaning OCR‚Äôd text with Regular Expressions](https://programminghistorian.org/en/lessons/cleaning-ocrd-text-with-regular-expressions)\n",
        "- [Regex cheat sheet ](https://www.rexegg.com/regex-quickstart.php)"
      ],
      "metadata": {
        "id": "xtxAa9aaugA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some examples of what you can do with regex.\n",
        "\n",
        "> Don't worry, you don't need to understand how this all works, but if you click the 'run' button at the left side of the code module it will run the code and show you the results.\n",
        "\n"
      ],
      "metadata": {
        "id": "-OYSoOSssxmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úèÔ∏è *Activity 4: Clean Your Text with Regex*  "
      ],
      "metadata": {
        "id": "55P-TGaAmkt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 1:\n",
        "- search and replace where long 's' with modern 's' characters.\n",
        "\n",
        "\"Thi≈ø i≈ø my extracted text, but it doe≈øn't look right\""
      ],
      "metadata": {
        "id": "AB3yRlUkvRxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import Python regex module re\n",
        "import re\n",
        "\n",
        "#identify the text we wish to correct\n",
        "test_text = \"Thi≈ø i≈ø my extracted text, but it doe≈øn't look right\"\n",
        "\n",
        "#this is the pattern we want to find with the long 's' character\n",
        "pattern = r\"≈ø\"\n",
        "\n",
        "#here we substitute the pattern (wrong character) for the right character - 'print' displays the corrections we have made\n",
        "print(re.sub(pattern, \"s\", test_text))"
      ],
      "metadata": {
        "id": "A0khEReLl_MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 2:\n",
        "\n",
        "Now we want to clean up the OCR file of the the image. Let's first have a look at the result."
      ],
      "metadata": {
        "id": "L-1whJ9l0Eqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "# Define the file to be imported\n",
        "filepath = \"https://raw.githubusercontent.com/DCS-training/OCR-From-Images-To-Text-2025/refs/heads/main/Working%20with%20Python/data/ocrtext_1.txt\"\n",
        "# Request the file from database\n",
        "req = requests.get(filepath)\n",
        "text = req.text\n",
        "\n",
        "# Print the text\n",
        "print(text)"
      ],
      "metadata": {
        "id": "NzwCjqFysq6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The OCR result is not too bad, but it still needs to be cleaned up with the following steps:\n",
        "\n",
        "1. join up words that are split across lines with '-'\n",
        "2. remove the '\"' at the begining of the sentences"
      ],
      "metadata": {
        "id": "tqjiRTKKzQBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace '\"' in the text with ''\n",
        "pattern1 = r'\"'\n",
        "result = re.sub(pattern1, '', text)\n",
        "print(\"Text after removing '\\\"':\\n\\n{} \\n\".format(result))"
      ],
      "metadata": {
        "id": "NCDlECsKtw72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace '-' in the text with ''\n",
        "pattern2 = r'-'\n",
        "result2 = re.sub(pattern2, '', result)\n",
        "print(\"Text after removing '-':\\n\\n{}\\n\".format(result2))"
      ],
      "metadata": {
        "id": "sM6DCSb_JSXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace newline '\\n' in the text with ' '\n",
        "pattern3 = r'\\n'\n",
        "result3 = re.sub(pattern3, ' ', result2)\n",
        "print(\"Text after removing 'nextline':\\n\\n{}\\n\".format(result3))"
      ],
      "metadata": {
        "id": "c6BLCqvXJSFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use \"OR\" character \"|\" and \"AND\" character \"&\" to combine different patterns\n",
        "# Replace '\"' or '-' or ''\\n in the text with''\n",
        "pattern = r'\"|-|\\n'\n",
        "result = re.sub(pattern, '', text)\n",
        "# Print the result\n",
        "print(\"Text after removing '\\\"':\\n\\n{} \\n\".format(result))\n"
      ],
      "metadata": {
        "id": "TXb01x5o4LPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also fix the formating by adding a blank space after the puctuation marks."
      ],
      "metadata": {
        "id": "Mn17PDiV46AY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following expression means \",\" or \";\" followed by (not blank space (?! )\n",
        "pattern4 = r'(,|;)(?! )'\n",
        "result_fixedpunctuation = re.sub(pattern4, ', ', result)\n",
        "# Print the result\n",
        "print(\"Text after fixing punctuation:\\n\\n{} \\n\".format(result_fixedpunctuation))"
      ],
      "metadata": {
        "id": "_hFx0bZr45Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üü¢ **OCR with Tesseract (pytesseract)**\n",
        "\n",
        "Tesseract is an OCR engine developed for various operating systems‚Äã to extract printed text from images. It is of the most accurate open-source OCR engines available‚Äã.\n"
      ],
      "metadata": {
        "id": "G74prlm1BKS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úèÔ∏è *Activity 5: pytesseract*  \n",
        "\n",
        "The following example demonstrate image pre-processing and text extraction using opencv and pytesseract.\n"
      ],
      "metadata": {
        "id": "mEE24sUpOlyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract\n",
        "!pip install deskew\n"
      ],
      "metadata": {
        "id": "GIs7bHE3BZ7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "!wget https://github.com/DCS-training/OCR-From-Images-To-Text-2025/raw/refs/heads/main/Working%20with%20Python/data.zip\n",
        "!unzip /content/data.zipA\n",
        "\n",
        "# These are the packages you will need to process the image and perform OCR scanning\n",
        "import pytesseract # OCR engine tesseract\n",
        "import cv2 # opencv\n",
        "import numpy as np # supporting open cv\n",
        "\n",
        "# To load language packages\n",
        "import os\n",
        "os.environ['TESSDATA_PREFIX'] = 'data/tessdata/'\n",
        "#print(pytesseract.get_languages(config='')) show available language packages\n",
        "\n",
        "# To plot and show image\n",
        "from skimage import io\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ],
      "metadata": {
        "id": "zFFZKu2wyc40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic text recognision with Tesseract"
      ],
      "metadata": {
        "id": "_C5TUJvUoDIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Run the following code to set up the files:"
      ],
      "metadata": {
        "id": "WoRv9M4vnt37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load and show image\n",
        "img = io.imread('data/img_english.jpg')\n",
        "io.imshow(img)"
      ],
      "metadata": {
        "id": "cGG7Ue67ksnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract text from img using \"image_to_string()\", the extracted text is stored in ocr_text\n",
        "#image_to_string() is a function in pytesseract,\n",
        "#it take an image as input and recognised text as output.\n",
        "\n",
        "ocr_text = pytesseract.image_to_string(img)\n",
        "print(ocr_text)"
      ],
      "metadata": {
        "id": "WSPucDLxpqs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text recognision in different/multiple languages\n",
        "\n",
        "Pytesseract uses English as its default language recognition package. However, more and more packages have been developed overthe years and you can find information and download the packages from the following website:\n",
        "\n",
        "- Check language package information: [Here](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html)\n",
        "- Download language packages: [Here](https://github.com/tesseract-ocr)\n",
        "- The most commonly used pre-trained language packages can be found in  tessdata_best folder\n"
      ],
      "metadata": {
        "id": "_5muVVdMpSVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# once you've downloaded the packages, you can check if they are installed successfully.\n",
        "# The following line shows all available tesseract language packages and codes\n",
        "print(pytesseract.get_languages(config=''))"
      ],
      "metadata": {
        "id": "tedKVKBXr8c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load and show image with Traditional Chinese texts\n",
        "img = io.imread('data/img_chi_tra.jpg')\n",
        "io.imshow(img)"
      ],
      "metadata": {
        "id": "HJI-HP1AvRLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract text by specifying the language package uisng argument lang=''\n",
        "# in our case, the language would be Traditional Chinese 'chi_tra'\n",
        "\n",
        "ocr_text = pytesseract.image_to_string(img, lang='') #the extracted text is stored in ocr_text\n",
        "print(ocr_text)"
      ],
      "metadata": {
        "id": "NSWatBn9veKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also specify multiple languages"
      ],
      "metadata": {
        "id": "P8clxzNGyFZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load and show image with multiple languages\n",
        "img = io.imread('data/img_multi.jpg')\n",
        "io.imshow(img)"
      ],
      "metadata": {
        "id": "IbgUlIISyonb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specifying multiple languages\n",
        "'''\n",
        "uisng argument lang='' to specify the language\n",
        "in the case of using multiple languages packages, use \"+\" in between the package codes\n",
        "for example: 'jpn+eng' means Japanese and English\n",
        "'''\n",
        "ocr_text = pytesseract.image_to_string(img, lang='') #the extracted text is stored in ocr_text\n",
        "print(\"\\nHere is the result:\\n\")\n",
        "print(ocr_text)"
      ],
      "metadata": {
        "id": "G4ndhF8UyRVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the the time taken for OCR as well as the output can be different based on the order of languages. If language is not specified, it would be English by default.\n",
        "\n",
        "Now let's try changing the order of the languages in lang argument and compare the results:"
      ],
      "metadata": {
        "id": "0gCZkiDfzBwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chi_text = pytesseract.image_to_string(img, lang='chi_tra')\n",
        "chijpn_text = pytesseract.image_to_string(img, lang='chi_tra+jpn')\n",
        "chijpneng_text = pytesseract.image_to_string(img, lang='chi_tra+jpn+eng')\n",
        "print(\"01 Chinese traditional:\\n\" + chi_text)\n",
        "print(\"\\n02 Chinese traditional + Japanese:\\n\" + chijpn_text)\n",
        "print(\"\\n03 Japanese + English:\\n\" + chijpneng_text)"
      ],
      "metadata": {
        "id": "hHehzAl8yycR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All of the results above has errors. Specifying segmentation is a common way to imrpove result accuracy as shown below."
      ],
      "metadata": {
        "id": "WYTwF5SA7l1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specifying segmentation\n",
        "\n",
        "By default Tesseract expects a page of text when it segments an image. If you‚Äôre just seeking to OCR a small region, try a different segmentation mode, using the --psm argument.\n",
        "\n",
        "```\n",
        "  0    Orientation and script detection (OSD) only.\n",
        "  1    Automatic page segmentation with OSD.\n",
        "  2    Automatic page segmentation, but no OSD, or OCR.\n",
        "  3    Fully automatic page segmentation, but no OSD. (Default)\n",
        "  4    Assume a single column of text of variable sizes.\n",
        "  5    Assume a single uniform block of vertically aligned text.\n",
        "  6    Assume a single uniform block of text.\n",
        "  7    Treat the image as a single text line.\n",
        "  8    Treat the image as a single word.\n",
        "  9    Treat the image as a single word in a circle.\n",
        " 10    Treat the image as a single character.\n",
        " 11    Sparse text. Find as much text as possible in no particular order.\n",
        " 12    Sparse text with OSD.\n",
        " 13    Raw line. Treat the image as a single text line,\n",
        "```\n",
        "- see full documentation [Here](https://tesseract-ocr.github.io/tessdoc/ImproveQuality.html#page-segmentation-method)\n",
        "- useful explanation of all options[here](https://pyimagesearch.com/2021/11/15/tesseract-page-segmentation-modes-psms-explained-how-to-improve-your-ocr-accuracy/)(some of them might not work properly)\n"
      ],
      "metadata": {
        "id": "5NrNJPZp4ECV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the previous example, it might help to specify that the texts are arranged in single lines of blocks:\n",
        "\n",
        "Try update the config arguement using config = \"--psm ?\" to get a better result"
      ],
      "metadata": {
        "id": "9TjIDAkE42Yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = io.imread('data/img_multi.jpg')\n",
        "io.imshow(img)\n",
        "ocr_text = pytesseract.image_to_string(img, lang='chi_tra+jpn',config=\"\") #the extracted text is stored in ocr_text\n",
        "print(\"\\nHere is the result:\\n\")\n",
        "print(ocr_text)"
      ],
      "metadata": {
        "id": "C3xgErKK0q99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use --psm 3 as suggested by the tesseract website\n",
        "img = io.imread('data/img_blocks.jpg')\n",
        "io.imshow(img)\n",
        "ocr_text = pytesseract.image_to_string(img, lang='eng',config=\"--psm 3\") #the extracted text is stored in ocr_text\n",
        "print(ocr_text)"
      ],
      "metadata": {
        "id": "HtFgoGe3_rTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The gap might be relatively small for the machine to recognise as separation between colunms\n",
        "# lets try resize the image to wider the gap\n",
        "height, width, colour = img.shape\n",
        "new_height = int(height*1)\n",
        "new_width = int(width*1.2)\n",
        "resized_image = cv2.resize(img, (new_width, new_height))\n",
        "io.imshow(resized_image)\n",
        "ocr_text = pytesseract.image_to_string(resized_image, lang='eng',config=\"--psm 3\") #the extracted text is stored in ocr_text\n",
        "print(ocr_text)"
      ],
      "metadata": {
        "id": "zyeURqSSGoWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colour correction\n",
        "\n",
        "Note that the UoE logo has a lighter colour which fails to be detected as characters. Another common way to improve ORC quality is coulour correction.\n",
        "\n",
        "Note\n",
        "### Colour convertion\n",
        "- COLOR_RGB2BGR remove alpha (transparent) chanell from image\n",
        "- COLOR_BGR2GRAY conver image to grey scale\n",
        "- Open cv [Colour conversion code](https://docs.opencv.org/3.4/d8/d01/group__imgproc__color__conversions.html)\n",
        "\n",
        "\n",
        "### Thresholding\n",
        "- Open cv [Threshholding](https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "xwZoD4WlHQh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use psm 3 as suggested by the official website\n",
        "img = io.imread('data/img_shadow.jpg')\n",
        "io.imshow(img)"
      ],
      "metadata": {
        "id": "6uYiNXIeHoL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "io.imshow(gray_img)\n"
      ],
      "metadata": {
        "id": "LrlaE9ASMsEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Thresholding method\n",
        "\n",
        "- The first argument is the source image, which should be a  grayscale image.\n",
        "- The second argument is the threshold value which is used to classify the pixel values.\n",
        "- The third argument is the maximum value which is assigned to pixel values exceeding the threshold.\n",
        "Note that we passed a combination of THRESH_BINARY + THRESH_OTSU\n",
        "THRESH_BINARY  turn the image in to either 0 (black) or 255(white)\n",
        "THRESH_OTSU The threshold value can be chosen arbitrary. The algorithm then finds the optimal threshold value which is returned as the first output.\n",
        "- The method returns two outputs. The first is the threshold that was used and the second output is the thresholded image."
      ],
      "metadata": {
        "id": "fyZgE2aLO8A9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the first output is the threshold value and the second is the thresholded image array\n",
        "ret, thresh = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "io.imshow(thresh) #show pre-processed image"
      ],
      "metadata": {
        "id": "Wp1GvkmxM0at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adaptive thresholding\n",
        "\n",
        "- The 1st argument is the source image, which should be a  grayscale image.\n",
        "- The 2nd argument is the threshold value which is used to classify the pixel values.\n",
        "- The 3rd argument is the adaptive thresholding method (here we use mean value method)\n",
        "- The 4th arguement is the threshold method (here we use binary method, same to simple thresholding)\n",
        "- The 5th our pixel neighborhood size, it must be an odd number\n",
        "- The 6th arguement let you tune the thresholding result. (more infromation on how to choose this constant [here](https://pyimagesearch.com/2021/05/12/adaptive-thresholding-with-opencv-cv2-adaptivethreshold/))"
      ],
      "metadata": {
        "id": "xBZuThQSPHz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#¬†Try different combination of arguement 5 and 6 and compare it with the reference results\n",
        "\n",
        "thresh = cv2.adaptiveThreshold(gray_img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, ? ,?)\n",
        "io.imshow(thresh)\n"
      ],
      "metadata": {
        "id": "JpNhbaImM3FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference result\n",
        "img = io.imread('data/results/thresh_result1.jpg')\n",
        "io.imshow(img)"
      ],
      "metadata": {
        "id": "jf2GejITUvMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ocr_text = pytesseract.image_to_string(thresh, lang='eng',config=\"--psm 3\") #the extracted text is stored in ocr_text\n",
        "print(ocr_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "WBVyP7BrQLg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key take away from this section\n",
        "\n",
        "Imgage pre-processing in ocr can improve the accuracy significantly. However, there is no universal solution or recipe to pre-process your data. It is important to observe your data and find the patterns and sources of errors that can be pre-processed.\n",
        "\n",
        "Reference parameter setting:\n",
        "\n",
        "\n",
        "- multiple language: --psm 4 (text with different sizes)\n",
        "- adaptive thresholding: pixcel size 25, constant c 20, psm 3"
      ],
      "metadata": {
        "id": "AW-cy0SJ6MwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 1: Japanese verticle text from old news paper\n",
        "\n",
        "\n",
        "\n",
        "> Please follow the following workflow to complete the task\n"
      ],
      "metadata": {
        "id": "DNENvlv77hgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 01 - Colour correction with thresholding technique\n",
        "Thresholding is the binarization of an image. We want to:\n",
        "1. convert a coloured image to grayscale image\n",
        "\n",
        "\n",
        "2. convert the grayscale image to a binary image, where the pixels are either 0 or 255\n",
        "\n",
        "This can be easily done with image processing library such as Opencv\n",
        "\n"
      ],
      "metadata": {
        "id": "XXh_8IGh-zjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import image processing and manipulation libraries\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "gxoHawRW-3DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load and show image\n",
        "img = io.imread('data/img_jpn_verticle.jpg')\n",
        "io.imshow(img)"
      ],
      "metadata": {
        "id": "yaroW0Jp_GIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cover image to grayscale using the same method mentioned above\n",
        "img = cv2.cvtColor(np.array(img), cv2.???)\n",
        "gray_img = cv2.cvtColor(img, cv2.???)\n",
        "io.imshow(gray_img)\n"
      ],
      "metadata": {
        "id": "eAeOtAWg_yCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "c6w8NU-ySaPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform simple thresholding\n",
        "ret, thresh = cv2.threshold(gray_img, 0, 255, ????)\n",
        "io.imshow(thresh) #show pre-processed image"
      ],
      "metadata": {
        "id": "BaGIRDnhA-WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference result\n",
        "img = io.imread('data/results/thresh_result2.jpg')\n",
        "io.imshow(img)"
      ],
      "metadata": {
        "id": "3nlAavG2U6bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 02 - Recognise verticle text\n",
        "For languages that use verticle writing system, there are language packages trained to tackle these variations.\n",
        "\n",
        "You can visit Tesseract documentation [page](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html) to check language package information and find the language package for recognising vertical japanese text.\n",
        "\n",
        "Also, you can play with different segmentation option and compare the results:\n"
      ],
      "metadata": {
        "id": "yGQPfQR9-1PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ocr_text = pytesseract.image_to_string(thresh, lang='', config=\"\")\n",
        "#the extracted text is stored in ocr_text\n",
        "print(\"\\nHere is the result:\\n\")\n",
        "print(ocr_text)\n"
      ],
      "metadata": {
        "id": "BCW0EbRjAsTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference results\n",
        "\n",
        "```\n",
        "ÂèØÊÑõ „ÅÑ 1 Âπ¥ Áîü\n",
        "\n",
        "Â∞èÂ≠¶ Ê†° „Åß ÊïôÂì° „Çí „Åó „Å¶ „ÅÑ„Çã „ÄÇ\n",
        "‰ΩìËÇ≤ „ÅÆ ÊéàÊ•≠ „Åß Ëª¢„Çì „Åß „Åó „Åæ„Å£ „Åü\n",
        "1 Âπ¥ Áîü „Å´ „Å∞ „Çì „Åù„ÅÜ „Åì„ÅÜ „Çí Ââá„Å£\n",
        "„Å¶ „ÅÇ„Åí „Åü „ÄÇ Êîæ Ë™≤ Âæå Â≠¶Á´• ‰øùËÇ≤\n",
        "„Å© „Å´ ÂØÑ„Çã „Åã Á¢∫Ë™ç „Åó „Åü „Åè „Å¶ [ ‰ªä\n",
        "Êó• „Åæ„Å£ „Åô„Åê Â∏∞„Çã ?„Äç „Å® ËÅû„Åè <\n",
        "„Å® „ÄÅ Ê≥£ „Åç „Å™ „Åå „Çâ „Äå „ÅÑ „Å™ V „Åà „ÄÅ „Å°\n",
        "„Çá „Å£ „Å® Êõ≤„Åå„Çä „Åæ„Åô „Äç„ÄÇ\n",
        "(Â∑ùÂ¥é Â∏Ç „Éª Êõ≤ „Åå „Çâ „Å™ „Åç„ÇÉ Â∏∞„Çå\n",
        "„Å™ „ÅÑ„ÇÇ „ÅÆ „Å≠ „Éª ÂÖ¨ Ê≠≥ )\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "K32dz-AATH-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 2: Meme with different colour background\n",
        "\n",
        "> Please follow the following workflow to complete the task\n"
      ],
      "metadata": {
        "id": "1Z6tkHbg8Za7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 01 - Colour selection with masking technique\n",
        "This example demonstrates a different colour convertion technique by creating a colour filter to make the range of colour of the text we want to extract from the image.\n",
        "\n"
      ],
      "metadata": {
        "id": "1v9XUQHq6Y0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = io.imread('data/img3.jpg')\n",
        "io.imshow(img)\n"
      ],
      "metadata": {
        "id": "ANOcWSvgShcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = pytesseract.image_to_string(img, lang='eng')\n",
        "print(text)"
      ],
      "metadata": {
        "id": "tcqLHFVGDwWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result shows that only the text \"Yellow Text with Black Background\" was extracted. Let's try increase the contrast of the colours with opencv."
      ],
      "metadata": {
        "id": "wAjxrg8hMEdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "HSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "plt.imshow(HSV)\n",
        "data = pytesseract.image_to_string(HSV, lang='eng')\n",
        "print(data)"
      ],
      "metadata": {
        "id": "DDfYICEtEdif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = cv2.inRange(HSV, (50, 255, 160), (200, 255, 255)) #Define the colour range\n",
        "imask = mask > 0 #Select the pixels where the colour is in the range\n",
        "result = np.zeros_like(img, np.uint8) #Create an empty image the same size to the file\n",
        "result[imask] = img[imask] #Copy the part of image in the colour range to the resulting image\n",
        "plt.imshow(result)\n",
        "data = pytesseract.image_to_string(result, lang='eng')\n",
        "print(data)"
      ],
      "metadata": {
        "id": "IE3oGVBEb9ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Try a different model to compare result:\n",
        "ocr_text = pytesseract.image_to_string(result, lang='eng', config = '--psm 11')\n",
        "print(ocr_text)"
      ],
      "metadata": {
        "id": "L6dabx0jc-kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02 Visualise the ocr result"
      ],
      "metadata": {
        "id": "ATQhLAkkSCz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some packages allows you to visualise the results:\n",
        "\n",
        "img = result\n",
        "hImg, wImg, _ = img.shape\n",
        "boxes = pytesseract.image_to_boxes(result,lang='eng', config = '--psm 11')\n",
        "#print(boxes)\n",
        "for b in boxes.splitlines():\n",
        "  b = b.split(' ')\n",
        "  x, y, w, h = int(b[1]), int(b[2]), int(b[3]), int(b[4])\n",
        "  rect = patches.Rectangle((x, hImg-y-(h-y)), w-x, h-y, linewidth=1, edgecolor='r', facecolor='none')\n",
        "  ax = plt.gca()\n",
        "  ax.add_patch(rect)\n",
        "\n",
        "plt.imshow(img)\n"
      ],
      "metadata": {
        "id": "lDZKtWHfSHec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Challenge yourself by extracting text from \"data/img4.jpg\""
      ],
      "metadata": {
        "id": "Xcv_1FICZfsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = io.imread('data/img4.jpg')\n",
        "io.imshow(img)\n"
      ],
      "metadata": {
        "id": "y2E72xCxZsZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 3: Skewed image\n",
        "\n",
        "Use the deskew library to correct the orientation and skewed texts. Please refere to the official document of deskew package: [here](https://pypi.org/project/deskew/)\n",
        "\n",
        "This library perform well on slightly skewed but clear scaned images.\n",
        "For people who are interested in tacking highly skewed images, please search for technique related to geometric distortion.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pFpVym3JFhyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Tuple, Union\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from deskew import determine_skew\n",
        "\n",
        "\n",
        "def rotate(\n",
        "        image: np.ndarray, angle: float, background: Union[int, Tuple[int, int, int]]\n",
        ") -> np.ndarray:\n",
        "    old_width, old_height = image.shape[:2]\n",
        "    angle_radian = math.radians(angle)\n",
        "    width = abs(np.sin(angle_radian) * old_height) + abs(np.cos(angle_radian) * old_width)\n",
        "    height = abs(np.sin(angle_radian) * old_width) + abs(np.cos(angle_radian) * old_height)\n",
        "\n",
        "    image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "    rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "    rot_mat[1, 2] += (width - old_width) / 2\n",
        "    rot_mat[0, 2] += (height - old_height) / 2\n",
        "    return cv2.warpAffine(image, rot_mat, (int(round(height)), int(round(width))), borderValue=background)\n",
        "\n"
      ],
      "metadata": {
        "id": "SxpnReW4G6Am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('data/img_skew.jpg')\n",
        "image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "ret, thresh = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "print(\"show original imgage\")\n",
        "io.imshow(thresh)"
      ],
      "metadata": {
        "id": "nLJ733_wW8EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "angle = determine_skew(thresh) #Determine correction angle\n",
        "print(\"Correction angle:\", angle)\n",
        "rotated = rotate(thresh, angle, (0, 0, 0))\n",
        "io.imshow(rotated)"
      ],
      "metadata": {
        "id": "5u8FNsddXGcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced application of Open CV and tesseract with line detection\n",
        "\n",
        "- [Edge and line detection](https://pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/)\n",
        "- [How to detect table](https://livefiredev.com/how-to-extract-table-from-image-in-python-opencv-ocr/)"
      ],
      "metadata": {
        "id": "3IkB8sVYVgu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üü¢ **Handwritten Text Recognition**\n",
        "\n",
        "**H**andrwitten **T**ext **R**ecognition (HTR) is another method of text extraction, although it is in the earlier stages of development than OCR is.\n",
        "Unlike OCR, which is best used on printed text, HTR engines are designed to be run on handwritten text and often use machine learning models to intelligently recognise text.\n"
      ],
      "metadata": {
        "id": "3dchfHlWiJm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úèÔ∏è *Activity 5*  \n",
        "\n",
        "[Transkribus](https://readcoop.eu/transkribus/) is one of the leading options in HTR; although designed for handwritten text, it can also be used on printed text.\n",
        "\n",
        "*   Try uploading an image (PNG or JPG) of handwritten and/or printed text to their website https://readcoop.eu/transkribus/\n",
        "\n",
        "*   How does changing the language used impact your results?\n",
        "\n",
        "\n",
        "**Disclaimer*: Transkribus operates on a paid credit model, but the test option outlined above is free. Sign up is free and includes a small number of credits when [joining](https://readcoop.eu/transkribus/credits/).\n",
        "\n"
      ],
      "metadata": {
        "id": "1bqz82-N2A1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-RhfmjXtOz8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Resources List**\n",
        "\n",
        "**Tutorials**\n",
        "\n",
        "Centre for Data, Culture and Society. \"Text Extraction & Preparation,\" Managing Digitised Documents (2022), https://www.cdcs.ed.ac.uk/training/training-pathways/managing-digitised-documents-pathway [accessed 23 July 2023].\n",
        "\n",
        "Knox, Doug. \"Understanding Regular Expressions,\" Programming Historian 2 (2013), https://doi.org/10.46430/phen0033 [accessed 23 July 2023].\n",
        "\n",
        "Turner O'Hara, Laura. \"Cleaning OCR‚Äôd text with Regular Expressions,\" Programming Historian 2 (2013), https://doi.org/10.46430/phen0024 [accessed 23 July 2023].\n",
        "\n",
        "Library Carpentry. \"Introduction to Working with Data (Regular Expressions)\" (2023), https://librarycarpentry.org/lc-data-intro/01-regular-expressions.html [accessed 23 July 2023].\n",
        "\n",
        "\"Regular Expressions Cheat Sheet\" by Davidchild, https://cheatography.com/davechild/cheat-sheets/regular-expressions/ [accessed 10 Sep 2025]\n",
        "\n",
        "**Readings**\n",
        "\n",
        "Cordell, Ryan. ‚Äò‚ÄúQ i-Jtb the Raven‚Äù: Taking Dirty OCR Seriously‚Äô. Book History 20, no. 1 (2017): 188‚Äì225. https://doi.org/10.1353/bh.2017.0006 [accessed 23 July 2023].\n",
        "\n",
        "Schantz, Herbert F. The History of OCR, Optical Character Recognition. [Manchester Center, Vt.]‚ÄØ: Recognition Technologies Users Association, (1982) http://archive.org/details/historyofocropti0000scha [accessed 23 July 2023].\n"
      ],
      "metadata": {
        "id": "Jh1bgAvnxJwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Activity Notes**\n",
        "\n",
        "\n",
        "## ‚úèÔ∏è Activity 1\n",
        "Even if you have not heard of OCR or are not sure if you have used it, you probably have! Here are some real-world examples of text recognition uses:\n",
        "\n",
        "*   Using the airport‚Äôs e-Passport Gates\n",
        "*   Translating text with language recognition\n",
        "*   Searching a digital database of historical public records\n",
        "*   Searching a PDF file\n",
        "\n",
        "## ‚úèÔ∏è Activity 2\n",
        "If you are not sure what to choose as your dataset, imagine you are digitising a book; think of your favourite book or try browsing the following website to find examples you could use:\n",
        "\n",
        "*   https://openbooks.is.ed.ac.uk/\n",
        "\n",
        "Don't forget that the type of materials you choose will impact your OCR, and different material types might need slightly different considerations for selecting OCR software or anticipating problems.\n",
        "\n",
        "\n",
        "\n",
        "## ‚úèÔ∏è Activity 3\n",
        "This list is not exhaustive but will cover some of the potential issues you might have picked out:\n",
        "\n",
        "Document 1:\n",
        "\n",
        "*   There are marks on the top left of the page that might be picked up by accident\n",
        "*   The text uses the long 's' form that may be mistaken for an 'f'\n",
        "*   You can see text coming through from other pages, depending on the OCR engine, this could be picked up\n",
        "*   Sometimes spacing between letters can cause issues, and the individual letters will be picked up rather than the entire word, for example with 'PETITION'\n",
        "\n",
        "Overall the image is clear, there is a good light balance and the text is quite straight on the page.\n",
        "\n",
        "Document 2:\n",
        "\n",
        "*   There are lots of marks and darlk spots on the page that may be picked up by the text recognition\n",
        "*   There are lots of creases in the page, meaning that some of the lettering is a little warped and may not be picked up correctly\n",
        "*   There is some text coming through from the other side of the page, although this is not as dark as in the previous example\n",
        "*   Some of the writing is very small and appears to be smudged in printing, which may mean that letters are misrecognised\n",
        "\n",
        "Overall, the text looks clear in some places, however the physical condition of the item is likely to impact the OCR done on this item due to the creases and marks on the page.\n",
        "\n",
        "## ‚úèÔ∏è Activity 4\n",
        "Any text extraction software or programme you decide to use will need to be evaluated - was it effective for the materials, or if not, why not? The limitations of OCR outputs are important to consider too - if you are only looking at a small quantity of text then you may be able to manually correct as much as you need to. If you are working with larger quantities of text this may not be the case and you will need to choose the best-performing option and do additional clean up with programming, or establish a suitable level of acceptable errors.\n",
        "\n",
        "## ‚úèÔ∏è Activity 5\n",
        "As shown in this example, image pre-processing can help improve the performace of OCR text extraction. However, when dealing with a large database, the type of the errors or \"noise\" in the data can vary and therefore requires a combination of techniques to extract the text successfully.\n",
        "\n",
        "\n",
        "## ‚úèÔ∏è Activity 6\n",
        "Transkribus has models designed for handwriting and print because they are trained on either handwritten text or printed text, to produce the vest results possible for the different types of materials. The same goes for different languages and some of the models distinguish between different fonts or handwriting types. If you play around with the model types and images you use, you should see a difference in output quality.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1qDfI1cqDHI3"
      }
    }
  ]
}